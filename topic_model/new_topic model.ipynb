{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle \n",
    "\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import os, re, operator, warnings\n",
    "warnings.filterwarnings('ignore')  # Let's not pay heed to them right now\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlitedict import SqliteDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pua_clean = SqliteDict(\"./../w2v/PUA clean.sqlite\", tablename=\"value\", flag=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pua_clean.values():\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating topic model by community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_modeling(sqlite_dict, sub):\n",
    "    dict_list = []\n",
    "    \n",
    "    for value in sqlite_dict.values():\n",
    "        if(value[\"text\"] != None):\n",
    "            dict_list.append(value)\n",
    "            dict_list.sort(key = lambda i: i['timestamp'])\n",
    "    \n",
    "    sub_sent=[value[\"text\"].split() for value in dict_list]\n",
    "    sub_phrases = Phrases(sub_sent, min_count=3)\n",
    "    sub_bigram = Phraser(sub_phrases)\n",
    "    sub_sentences = sub_bigram[sub_sent]\n",
    "    \n",
    "    with open(f\"./data/{sub}_sentences.txt\", \"wb\") as fp:  # Pickling\n",
    "        pickle.dump(sub_sentences, fp)\n",
    "    print(f'{sub}_sentences.txt created')\n",
    "    \n",
    "    bigram = gensim.models.Phrases(sub_sentences)\n",
    "\n",
    "    dictionary = Dictionary(sub_sentences)\n",
    "    dictionary.save(f\"./data/{sub}_hdp_dictionary.dict\")\n",
    "    \n",
    "    print(f\"{sub} Dictionary saved as {sub}_hdp_dictionary.dict\")\n",
    "    corpus = [dictionary.doc2bow(text) for text in sub_sentences]\n",
    "    MmCorpus.serialize(f'./data/{sub}_hdp_corpus.mm', corpus)\n",
    "    print(f'Corpus saved as {sub} hdp_corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUA_sentences.txt created\n",
      "PUA Dictionary saved as PUA_hdp_dictionary.dict\n",
      "Corpus saved as PUA hdp_corpus.mm\n"
     ]
    }
   ],
   "source": [
    "topic_modeling(pua_clean, 'PUA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_request = [(k, v[\"text\"]) for k, v in itertools.islice(db1.items(), args.init, args.end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_topic_creation(model_sqlite, community):\n",
    "    \n",
    "    models = []\n",
    "    years = []\n",
    "    \n",
    "    sub = community\n",
    "    \n",
    "    for value in model_sqlite.values():\n",
    "        dt_object = datetime.fromtimestamp(value[\"timestamp\"]//1000)\n",
    "        if dt_object.year not in years:\n",
    "            years.append(dt_object.year)\n",
    "    \n",
    "    print(years)\n",
    "    for year in years:\n",
    "        sub_sent = []\n",
    "        for value in model_sqlite.values():\n",
    "            dt_object = datetime.fromtimestamp(value[\"timestamp\"]//1000)\n",
    "            if dt_object.year == year and value[\"text\"] != None:\n",
    "                sub_sent.append(value[\"text\"].split())\n",
    "        \n",
    "        sub_phrases = Phrases(sub_sent, min_count=30)\n",
    "        sub_bigram = Phraser(sub_phrases)\n",
    "        sub_sentences = sub_bigram[sub_sent]\n",
    "\n",
    "        with open(f\"./data/{sub}_sentences_{year}.txt\", \"wb\") as fp:  # Pickling\n",
    "            pickle.dump(sub_sentences, fp)\n",
    "        print(f'{sub}_sentences_{year}.txt created')\n",
    "\n",
    "        bigram = gensim.models.Phrases(sub_sentences)\n",
    "\n",
    "        dictionary = Dictionary(sub_sentences)\n",
    "        dictionary.save(f\"./data/{sub}_hdp_dictionary_{year}.dict\")\n",
    "\n",
    "        print(f\"{sub} Dictionary saved as {sub}_hdp_dictionary_{year}.dict\")\n",
    "        corpus = [dictionary.doc2bow(text) for text in sub_sentences]\n",
    "        MmCorpus.serialize(f'./data/{sub}_hdp_corpus_{year}.mm', corpus)\n",
    "        print(f'Corpus saved as {sub}_hdp_corpus_{year}.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oi\n",
      "[2019, 2018, 2017, 2016]\n",
      "PUA_sentences_2019.txt created\n",
      "PUA Dictionary saved as PUA_hdp_dictionary_2019.dict\n",
      "Corpus saved as PUA_hdp_corpus_2019.mm\n",
      "PUA_sentences_2018.txt created\n",
      "PUA Dictionary saved as PUA_hdp_dictionary_2018.dict\n",
      "Corpus saved as PUA_hdp_corpus_2018.mm\n",
      "PUA_sentences_2017.txt created\n",
      "PUA Dictionary saved as PUA_hdp_dictionary_2017.dict\n",
      "Corpus saved as PUA_hdp_corpus_2017.mm\n",
      "PUA_sentences_2016.txt created\n",
      "PUA Dictionary saved as PUA_hdp_dictionary_2016.dict\n",
      "Corpus saved as PUA_hdp_corpus_2016.mm\n"
     ]
    }
   ],
   "source": [
    "time_topic_creation(pua_clean, \"PUA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_time_model_creation(sub, years=[2018, 2019]):\n",
    "    for year in years:\n",
    "        with open(f\"./data/{sub}_sentences_{year}.txt\", \"rb\") as fp:   # Unpickling\n",
    "            sub_sentences = pickle.load(fp)\n",
    "            \n",
    "        dictionary = Dictionary.load(f'./data/{sub}_hdp_dictionary_{year}.dict')\n",
    "        corpus = MmCorpus(f'./data/{sub}_hdp_corpus_{year}.mm')\n",
    "                  \n",
    "        ldamodel = LdaModel(corpus=corpus, num_topics=15, id2word=dictionary)\n",
    "        ldamodel.save(f'./lda/{sub}_lda_model_{year}.gensim')\n",
    "        print(f'{sub} lda model created as {sub}_lda_model_{year}.gensim')\n",
    "\n",
    "        ldatopics = [[word for word, prob in topic] for topicid, topic in ldamodel.show_topics(formatted=False)]\n",
    "\n",
    "        lda_coherence = CoherenceModel(topics=ldatopics[:10], texts=sub_sentences,\n",
    "                                       dictionary=dictionary, window_size=10).get_coherence()\n",
    "\n",
    "        print(f\"The topic coherence is {lda_coherence}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUA lda model created as PUA_lda_model_2018.gensim\n",
      "The topic coherence is 0.3018479142328555\n",
      "\n",
      "PUA lda model created as PUA_lda_model_2019.gensim\n",
      "The topic coherence is 0.2615656517145621\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LDA_time_model_creation('PUA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_model_creation(sub):\n",
    "        \n",
    "    with open(f\"./data/{sub}_sentences.txt\", \"rb\") as fp:   # Unpickling\n",
    "            sub_sentences = pickle.load(fp)\n",
    "            \n",
    "    dictionary = Dictionary.load(f'./data/{sub}_hdp_dictionary.dict')\n",
    "    corpus = MmCorpus(f'./data/{sub}_hdp_corpus.mm')\n",
    "\n",
    "    ldamodel = LdaModel(corpus=corpus, num_topics=15, id2word=dictionary)\n",
    "    ldamodel.save(f'./lda/{sub}_lda_model.gensim')\n",
    "    print(f'{sub} lda model created as {sub}_lda_model.gensim')\n",
    "\n",
    "    ldatopics = [[word for word, prob in topic] for topicid, topic in ldamodel.show_topics(formatted=False)]\n",
    "\n",
    "    lda_coherence = CoherenceModel(topics=ldatopics[:10], texts=sub_sentences,\n",
    "                                   dictionary=dictionary, window_size=10).get_coherence()\n",
    "\n",
    "    print(f\"The topic coherence is {lda_coherence}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PUA lda model created as PUA_lda_model.gensim\n",
      "The topic coherence is 0.3553723479661005\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LDA_model_creation('PUA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2016, 2017, 2018, 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.wrappers.dtmmodel import DtmModel\n",
    "PUA_corpus = MmCorpus('./data/PUA_hdp_corpus.mm')\n",
    "PUA_dict =  Dictionary.load('./data/PUA_hdp_dictionary.dict')\n",
    "PUA_time_seq = []\n",
    "for year in years:\n",
    "    c = MmCorpus(f'./data/PUA_hdp_corpus_{year}.mm')\n",
    "    PUA_time_seq.append(len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 529, 4578, 3149]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PUA_time_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_path = \"./../../_PushshiftReddit/topic_model/data/dtm/dtm-linux64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUA_model = DtmModel(dtm_path, PUA_corpus, PUA_time_seq, num_topics=10,\n",
    "                 id2word=PUA_dict, initialize_lda=True, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUA_model.save(\"./lda/PUA_dtm.gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
